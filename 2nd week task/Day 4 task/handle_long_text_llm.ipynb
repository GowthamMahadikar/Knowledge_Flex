{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fcbc7a",
   "metadata": {},
   "source": [
    "# Handling Long Text in LLMs\n",
    "This notebook demonstrates practical techniques to handle long text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955aad5b",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73deb103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1:\n",
      "Large Language Models have a limited context window. When documents exceed this limit, they must be split into chunks. Chunking\n",
      "\n",
      "Chunk 2:\n",
      "be split into chunks. Chunking helps process long documents like PDFs, logs, and books. Each chunk is processed independently by\n",
      "\n",
      "Chunk 3:\n",
      "chunk is processed independently by the LLM.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=100, overlap=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Example long text\n",
    "long_text = \"\"\"\n",
    "Large Language Models have a limited context window.\n",
    "When documents exceed this limit, they must be split into chunks.\n",
    "Chunking helps process long documents like PDFs, logs, and books.\n",
    "Each chunk is processed independently by the LLM.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_text(long_text, chunk_size=20, overlap=5)\n",
    "\n",
    "for i, c in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\\n{c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781ed12",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12584b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window 1:\n",
      "Large Language Models have a limited context window. When documents exceed this limit, they must\n",
      "\n",
      "Window 2:\n",
      "limited context window. When documents exceed this limit, they must be split into chunks. Chunking\n",
      "\n",
      "Window 3:\n",
      "exceed this limit, they must be split into chunks. Chunking helps process long documents like\n",
      "\n",
      "Window 4:\n",
      "be split into chunks. Chunking helps process long documents like PDFs, logs, and books. Each\n",
      "\n",
      "Window 5:\n",
      "helps process long documents like PDFs, logs, and books. Each chunk is processed independently by\n"
     ]
    }
   ],
   "source": [
    "def sliding_window(text, window_size=15, stride=5):\n",
    "    words = text.split()\n",
    "    windows = []\n",
    "\n",
    "    for i in range(0, len(words) - window_size + 1, stride):\n",
    "        window = \" \".join(words[i:i + window_size])\n",
    "        windows.append(window)\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "windows = sliding_window(long_text)\n",
    "\n",
    "for i, w in enumerate(windows, 1):\n",
    "    print(f\"\\nWindow {i}:\\n{w}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bba3b",
   "metadata": {},
   "source": [
    "## Hierarchical Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "363b4e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary:\n",
      " Summary: Large Language Models have a limited context window. When do... Summary: be split into chunks. Chunking helps process long documents ... Summary: chunk is processed independently by the LLM....\n"
     ]
    }
   ],
   "source": [
    "chunk_summaries = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    summary = f\"Summary: {chunk[:60]}...\"\n",
    "    chunk_summaries.append(summary)\n",
    "\n",
    "final_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "print(\"Final Summary:\\n\", final_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ea990",
   "metadata": {},
   "source": [
    "## Retrieval-Based Long Text Handling (Mini RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53135183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "- Large Language Models have a limited context window. When documents exceed this limit, they must be split into chunks. Chunking\n"
     ]
    }
   ],
   "source": [
    "def retrieve_chunks(query, chunks):\n",
    "    relevant = []\n",
    "    for chunk in chunks:\n",
    "        if any(word.lower() in chunk.lower() for word in query.split()):\n",
    "            relevant.append(chunk)\n",
    "    return relevant\n",
    "\n",
    "\n",
    "query = \"context window\"\n",
    "results = retrieve_chunks(query, chunks)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for r in results:\n",
    "    print(\"-\", r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de580e15",
   "metadata": {},
   "source": [
    "## Long Conversation Memory Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65dc5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation summary: User asked about LLMs and context window.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    \"User: Explain LLMs\",\n",
    "    \"Assistant: LLMs are large neural networks\",\n",
    "    \"User: What is context window?\",\n",
    "    \"Assistant: It limits how much text can be read\"\n",
    "]\n",
    "\n",
    "def summarize_conversation(messages):\n",
    "    return \"Conversation summary: User asked about LLMs and context window.\"\n",
    "\n",
    "summary = summarize_conversation(conversation)\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56eeb0-eaea-458d-91f1-b4ee29f2af31",
   "metadata": {},
   "source": [
    "## Combined Real-World Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36592d26-8c8c-40ec-9fd9-003003f73f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      " Large Language Models have a limited context window. When documents exceed this limit, they must be split into chunks. Chunking helps process long documents like PDFs, logs, and books. Each chunk is processed independently by the LLM.\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text(long_text)\n",
    "retrieved = retrieve_chunks(\"documents\", chunks)\n",
    "\n",
    "final_answer = \" \".join(retrieved)\n",
    "print(\"Final Answer:\\n\", final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40863f-898b-432c-b2c6-91767e41481a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
