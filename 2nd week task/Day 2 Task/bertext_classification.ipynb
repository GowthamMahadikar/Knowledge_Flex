{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Classification using Pre-trained BERT\n",
        "\nThis notebook demonstrates sentiment classification using a pre-trained BERT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install torch transformers scikit-learn pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = {\n",
        "    'text': [\n",
        "        'I love this product',\n",
        "        'This is the worst experience',\n",
        "        'Amazing quality and service',\n",
        "        'Very bad and disappointing',\n",
        "        'I am happy with the result',\n",
        "        'Not worth the money'\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx], padding='max_length', truncation=True,\n",
        "            max_length=self.max_len, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "print('Accuracy:', accuracy_score(true_labels, predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict_sentiment(text):\n",
        "    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1)\n",
        "    return 'Positive' if prediction.item() == 1 else 'Negative'\n\n",
        "predict_sentiment('The product quality is excellent')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}